Function Copy-SQLDatabase {

	[CmdLetBinding(SupportsShouldProcess=$True)]
	param(
		#SOURCE OPTIONS
			 #The source instance
				$SourceServerInstance = $null
			,#The source database name
				$SourceDatabase = $null
			,#If you use a existing backup to restore, instead of take a new, use this parameter to specify the full path of this backup.
				$ExistentBackupName = $null
			,#If specified, cmdlet will try get most recently backup in folder specified by BackupFolder.
				[switch]
				$UseRecentBackup = $false
			,#Its the behavior of cmdlet when getting existent file to restore.
			 #Possible values are: GenerateIfDontExists | MustUse
			 #	GenerateIt fDontExist tells the cmdlet to generate backup if file dont exists.
			 #	MustUse tells the cmdlet to stops and raise error if file dont exists.
				[string]
				$UseExistentPolicy = "GenerateIfDontExists"
			,#This parameters controls the how much recently a backup must considered for UseRecentBackup. Just files with CreateDate grather or equals this will be considered.
			 #When NULL, then not time is considered.
				[Object]
				$RecentBase  = $null
			,#This controls recent backup filename. By default, scripts finds by backups with same internal filename structure generated by command.
			 #You can use this to force script find by specific files when looking for a recent one.
				[string]
				$RecentFileMask = $null
				
			,#There are many backup sources. You can control the order in which the script will try using a source.
			 #The RIGHTMOST option is try before the leftmost. If it execute with sucess and get the source information, then, it stops try getting a source.
			 #You can change this behavior with next parameters
			 #The accepatable sources types are: E (For $ExistentBackup) R (For $UseRecent) and S (For SourceServerInstance/SourceDatabase)
			 #If you hide a option, then script will not try use this option.
			 #If script try use all options specified without sucess, a exception is throwed.
				[string[]]
				$SourceOrder = @("E","R","S")
				
			,#Puts database in readonly before take backup. This is useful for migrating database!
				[switch]
				$SourceReadOnly	= $false
				
			,#Allows you specify a custom source backup command. 
			 #You must return a resultset with a single column called "backupfile" for the script use it to restore.
			 #This option is useful, if some error happening with script or if some option is not supported still by script.
				[string]
				$BackupTSQL = $null
				
		#DESTINATION OPTIONS
			,#The destination instance
				$DestinationServerInstance
			,#The destination database name
				$DestinationDatabase
			,#Specify a destination location to backup database the destination database, before replace it.
			 #If null, or if destination dont exists, no backup is taken on destination.
			 #THe script consider that destination is a folder location and file will generate the filename pattern!
				[string]
				$DestinationDatabaseBackup = $null
				
			,#If specified, the script will read all permissions on destination database if it exists.
			 #After restored, the script will try re-apply permissions. Any erros on this step dont break execution and are logged.
			 #System permissions and user are not considered.
				[switch]
				$KeepPermissions = $true
				
			,#Dump de DCL of permissions backed up to the specified file.
			 #It is useful if the restore fails in middle of process.
				[string]
				$ExportPermissionsFile = $null
			
		#BACKUP OPTIONS
			,#The folder where backup are be taken. Can be share. 
				$BackupFolder
			,#The folder where backup will be restored. If same of BackupFolder, can be NULL.
				$RemoteBackupFolder = $null		
				
			,#If specified, the cmdlet will apend a timestamp to backup file to make a unique backup filename.
				[switch]
				$UniqueBackupName = $false

		#DATABASE FILES OPTIONS
			,#By default, if a destination database name exists, this cmdlet try put files on same location as existing (renaming) 
			 #If this parameters is used, it will no try this.
				[switch]
				$NoUseCurrentFilesFolder = $false

			,#Its the behavior of cmdlet when restauring to same location. 
			 #Possible valures are: ReplaceIfExists | MustReplace
			 #	ReplaceIfExists tells the cmdlet that if database dont exists, the volume distribution must be used.
			 #	MustReplace tells the cmdlet that database must be replaced. If dont exists, then it fails!
				[string]
				$CurrentFilesFolderPolicy = "ReplaceIfExist"

			<#
			,#If specified, th cmdlet will not rename files (if existing. See NoUseCurrentFilesFolder parameter)
				[switch]
				$NoRenameFiles = $false
			#>
			
			,#If specified, no file mapping will be taken and this cmdlet will try restore without any MOVE option.
				[switch]
				$NoRelocFiles = $false
				
			,#Provide a manal file mapping to restore command. This will ignore all other options relative to files.
			 #This must be a hashtable array. Each hashtable must be following properties:
			 #	logicalName (the logical name of file) , physicalName (the full file path to map)
				$ManualFileMapping = $null
				
			,#The folder structure which put the database files. This must exists.
				$RestoreFolder=$null
				
			,#Volume to folder mapping...
			 #The following members are expected: 
			 # volumeName (the volume path), path (the folder path), freeLimit (Amount of free limite to be on volume)
				[hashtable[]]
				$VolumeToFolder=@()
			,#If specified , then cmdlet will ignore existing files and LUNS and will use the mapping provide in VolumesToFolder parameter.
			 #With this, you are  bypassing volume detection and say
				[switch]
				$ForceVolumeToFolder = $false
			,#Each element is a volume name. 
			 #It can accept wildcard to be used the ps -like operator
				[string[]]
				$VolumesAllowed=@() 
			,#Volumes allowed for data files.
				[string[]]
				$VolumesForData=@()
			,#Volumes allowed for log files
				[string[]]
				$VolumesForLog=@()
				
			,#If destination instance supports, query avaiable volumes using DMVs.
				[switch]
				$GetVolumesInSQL = $false
				
			<#
			,#Indicates the how much free space must be left on a voluem when analyzing free space to put a database file.
			 #You can specify a fixed size or a percentage. The percentage will be calculted based on full size of volume.
			 #If null, the no limit is applied.
			 #Examples: 1%, 1MB, 20GB, 10MB, 100B.
				[string]
				$VolumeFreeLimit = $null
				
			#>


		#RESTORE OPTIONS
			,#Its same as tsql REPLACE option.	
				[switch]
				$Replace = $false	

			,#Its same as NORECOVERY option. Note that with this option, any actions to be taken on restored database will fail. (E.g Post scritps)
				[switch]
				$NoRecovery = $false
				
			,#Puts database in SIMPLE recovery model after restore. This is useful when copy for non production environment.
				[switch]
				$ForceSimple = $false
				

				
		#SCRIPTING OPTIONS
			,#Specify scripts must be executed after restore is completed.
			 #You can specify a array of scripts, where each element is executed in single batch.
			 #The parameter "PostScriptPolicy" controls behavior when scripts fails. 
			 #You can specify a script or filename. For filename, just put prefix "FILE:" before. If file not exists, this will result in error...
				[string[]]
				$PostScripts = @()
			,#Specify behavior when applying post scripts. The default is stop scripts on errors...
			 #Possible values are: 
			 #	StopOnError: The apply ends erros message is logged. 
			 #	SkipErrors: If a error is ocurred, the erros is logged and just current batch ends, and next continue.
				[string]
				$PostScriptPolicy = "StopOnError"
				
			,#Determines that scripts must run under account with permissions only in database.
			 #This will create login and user on destination database and run scripts under this permissions.
			 #This prevent scripts be executed with elevated permissions
				[switch]
				$UseLimitedUser = $false
				
			,#Determine actions that be take if limited user nad login already exists.
			 #By default, limited user is Created with a name "CopySQLDatabaseLimited_<DestinationDatabase>"
			 #This parameter determine actions to be taken if user or login already exists
			 #	DropIfExists: Drop  if it already exists.
			 #	MustCreate: Login/User must be created. If user already exists, a exception is throwed.
				[string]
				$LimitedUserPolicy = "DropIfExist" 
			
		#LOGGING OPTIONS
			,#Log level to display. This controls how much logging will be write to log destinations.
			 #For all possible levels run Get-LogLevels cmdlet (this cmdlet is availble when importing module CustomMSSQL)
				$LogLevel = "DETAILED"
			,#Specify log destinations. You can pass a array of logs. NOTE: More destinations, more slow cmdlet will run.
			 #A special destination is "#". This tells to cmdlet to log to screen (host).
			 #If you pass Verbose parameter, the screen will be replaced by verbose output.
				$LogTo = "#"
				
			,#Buffers all logging message and when scripts runs. output it.
			 #This is useful when running this script via SQL Server agent.	
			 #Note this just bufferizes host output, but not bufferize file output...
				[switch]
				$BufferLog = $false
				
			,#Log script start message directly, same if buffering is enabled.
				[switch]
				$ForceStartMessage = $true
				
			,#SQL Agent error log mode. 
			 #This force specific behavior on logging and error output to be correctly catched by SQL Agent interface.
			 #If this option is used, BufferLog and ForceStartMessage is set to true, independetly user choice.
			 #Also, when this option is set, the script sets the DOS error level to a number other than 0 for SQL Agent alert correct states of script.
				[switch]
				$SQLAgentErrorMode = $false
				
		#MISC AND DEBUGGING OPTIONS
		
		
		,#Suggest only option.
		 #Dont execute any modification commands, how backups or restores, and just suggest.
		 #This option dont execute backup/restore, or post scripts, command, but check files at source instance for build report.
			[switch]
			$SuggestOnly = $false
			
		,#By default, SuggesTOnly disable all logging because scripts assume that user requires just information on screen.
		 #With this option, you can turn logging normal as you configured with loggin parameters.
			[switch]
			$ForceLogging=$false
			
	)
	
	$ErrorActionPreference = "Stop";
	$IsVerbose = $PSCmdlet.MyInvocation.BoundParameters["Verbose"].IsPresent
	
	try {
		#Defining some functions and configurations for script...
			
			$GMV = GetGMV
			#Shared object containing all values used in script.
			#Any part of this script must use this variable to get some value. This will be documented help of this cmdlet. 	

			$VALUES = @{
				CORESCRIPTS_FOLDER	= "$($GMV.CMDLETSIDR)\Copy-SQLDatabase\ps" 	
				DFA_FOLDER 			= "$($GMV.CMDLETSIDR)\Copy-SQLDatabase\ps" 		
				DFA_EXTENSION		= "distalg.ps1"
				SBA_FOLDER			= "$($GMV.CMDLETSIDR)\Copy-SQLDatabase\ps\SBA"  #Source Backup Algorithm
				SBA_EXTENSION		= "sba.ps1" 	
				DFA 				= @{}
				PARAMS 				= (GetAllCmdLetParams)
				DFA_TOEXEC			= $NULL
				CURRENT_LOCATION	= ($PsScriptRoot)
				SQL_SCRIPTS_FOLDERS		= "$($GMV.CMDLETSIDR)\Copy-SQLDatabase\sql"							
				SCRIPT_STORE		= @{
										SQL	 = @{}
										FUNCTIONS = @{}	
										SBA = @{}
										
										#Contains all scripts powershell in \Copy-SQLDatabase\ps folder that ends with "core.ps1".
										#This type of routine handles specific parts of entire process.
										CORE=@{} 
									}
									
				LIMITED_USER		= @{NAME=$null;PASSWORD=$null;USER_TYPE="SQL"}
				EXCEPTIONSTABLE_FILE	= "$($GMV.CMDLETSIDR)\Copy-SQLDatabase\Exceptions.ex.ps1"
				EXCEPTIONS				= @()
				SUGGEST_REPORT			= @{}
				SOURCE_BACKUP			= $null
				BACKUP_FILEPREFIX		= $null
				BACKUP_FILESUFFIX		= $null
				SQLINTERFACE			= $null
				WIN_USERNAME			= [System.Security.Principal.WindowsIdentity]::GetCurrent().Name
				COMPUTER_NAME			= $Env:ComputerName
				DESTINATION_INFO		= @{DestinationFiles=$null}
			}
			
			#This is for internal and undocumented tests only...
			$VALUES.PARAMS.add("WaitOn","SourceBackup")
			
			#Loading the exeption table... This is not fully implented yet...
			if(Test-Path $VALUES.EXCEPTIONSTABLE_FILE){
				$VALUES.EXCEPTIONS = & $VALUES.EXCEPTIONSTABLE_FILE;
			}
			
			#Hanlding back slash on parameters that contains a folder
			$BackupFolder = (PutFolderSlash $BackupFolder)
			$RemoteBackupFolder = (PutFolderSlash $RemoteBackupFolder)
			$RestoreFolder = (PutFolderSlash $RestoreFolder)
			
			$VolumeToFolder | where {$_}  | %{
				$_.volumeName 	= (PutFolderSlash $_.volumeName)
				$_.path 		= (PutFolderSlash $_.path)
			}	
			
			#Adjusting for SQL Agent Error Mode...
			if($SQLAgentErrorMode){
				$BufferLog = $true;
				$ForceStartMessage = $true;
			}
			
			#Creating necessary objects for use facilities providers by Logging from CustomMSSQL module.
			$Log = (GetLogObject)
			$Log.LogTo = $LogTo;
			$Log.LogLevel = $LogLevel; 
			$Log.ExternalVerboseMode = $IsVerbose; 
			
			#If buffer log is enabled, then bufferize logging to hosts powershell application....
			if($BufferLog){
				$Log.LogTo += "#BUFFER"
			}
			
			#If cmdlet is in verbose mode, force verbose logging....
			if($IsVerbose){..
				$Log.LogLevel = "VERBOSE";
			}

			#Creates a easy call logging functions to be used on this script....
			Function Log {
				param($message,$LogLevel = $null,$Force = $false, $Retain = $false, $Flush = $false)
				
				if($SuggestOnly -and !$ForceLogging){
					return;
				}
				
				$Options = @{retain=$Retain;flush=$Flush};
				

				
				if($message -is [string])
				{
					#write-host $message
					$Log.Log($message,$LogLevel,$Force,$null,$null,$Options)
					
				} else {
					$Log.LogSQLErrors($message,$LogLevel,$Force);
				}

			}
			
			#Register function in global variables...
			$VALUES.SCRIPT_STORE.FUNCTIONS.add("Log","Log");
			
			#This functions allows script call cores scripts that complements cmdlet work.
			#This handles repetitive tasks...
			Function CallCoreScript {
				param($CoreScriptName,[switch]$LogErrorOnly = $false)
				
				try {
					if(!$VALUES.SCRIPT_STORE.CORE.Contains($CoreScriptName)){
						throw "INVALID_CORE_SCRIPTNAME"
					}
					
					& $VALUES.SCRIPT_STORE.CORE.$CoreScriptName $VALUES;
					
				} catch {
					Log "CORE_SCRIPT_ERROR[$CoreScriptName]: $_"
					if(!$LogErrorOnly){throw;}
				}
			}

		#Log start of script, forcing no buffering if specified by user.
		Log "Starting script!. Log Level is: $($Log.LogLevel). Current computer name is $($VALUES.COMPUTER_NAME). Current User is $($VALUES.WIN_USERNAME)" -Force $ForceStartMessage
		
		#Logging parameters invoked!
		Log "SCRIPT EFECTIVE PARAMETERS: " -Force $ForceStartMessage  -Retain $true
			$VALUES.PARAMS.GetEnumerator() | %{
				Log "	$($_.Key): $($_.Value)"
			}
			Log "" -Force $ForceStartMessage -Flush $true #Just for flush log messages retained., if users wants force initial messages!
		
		
		#Loading core scripts. Core scripts are scripts that implements some functionality of cmdlets.
		Log "Loading core scripts ..."
			$CoreFolder = (PutFolderSlash $VALUES.CORESCRIPTS_FOLDER)+"*.core.ps1"
			gci $CoreFolder | %{
				Log "	Loading core script $($_.FullName) " "VERBOSE"
				$CoreScriptName = $_.Name.replace(".core.ps1","")
				Log "	CORE SCRIPT IS: $CoreScriptName" "VERBOSE"
				$VALUES.SCRIPT_STORE.CORE.add($CoreScriptName,$_.FullName);
			}
			Log "	Cores Scripts: $($VALUES.SCRIPT_STORE.CORE.Count)"

		#The distribution algorithms, called "DFA", are scripts that handles logic to distribute database files to disks.
		Log "Loading distribute algorithms ..."
			$algFilter = (PutFolderSlash $VALUES.DFA_FOLDER)+"*."+($VALUES.DFA_EXTENSION)
			Log "	FILES: $algFilter" "VERBOSE"
			gci $algFilter | %{
				Log "	Loading file distributor algorigthm  $($_.FullName) " "VERBOSE"
				$AlgName = $_.Name.replace("."+$VALUES.DFA_EXTENSION,"")
				Log "	ALGORITHM IS: $AlgName" "VERBOSE"
				$VALUES.DFA.add($AlgName,$_.FullName);
			}
			#Set defaults DFA...
			$VALUES.DFA_TOEXEC = "BIGGERS_FIRST";
			
			$DfaCount = $VALUES.DFA.Count;
			
			if(!$DfaCount){
				throw "NO_DFA"
			}
			
			Log "	DFA Scripts: $DfaCount"
			
		#The SQL scripts provide all code necessary to get information or execute tasks in involed SQL Server instances.
		Log "Loading SQL Scripts..."
			$sqlScriptsFilter = (PutFolderSlash $VALUES.SQL_SCRIPTS_FOLDERS)+"*"
			gci $sqlScriptsFilter | %{
				Log "	Getting SQL Script $($_.FullName)" "VERBOSE"
				$ScriptType = [System.IO.Path]::GetExtension($_.Name)
				$ScriptName = $_.BaseName
				
				Log "	SCRIPT WILL IDENTIFIED BY ($($ScriptType)): $ScriptName" "VERBOSE"
				
				$ScriptContent = $null;
				#If a SQL Script, then assume it contains direct SQL code with
				if($ScriptType -eq ".sql"){
					Log "	SCRIPTS IS A SQL FILE. CONVERTING TO A SCRIPTBLOCK..." "VERBOSE"
					$ScriptText = Get-Content $_.FullName | Out-String
					$ScriptContent = [scriptblock]::create({return $ScriptText}).GetNewClosure();
				} else {
					$ScriptContent = $_.FullName;
				}

				$VALUES.SCRIPT_STORE.SQL.add($ScriptName,$ScriptContent);
			}
			
		#Source backup algorithms are scripts that generates a source backup which will be used to restore database in the destination.
		Log "Loading source backup algorithms ..."
			#Loads the source backups algoithms. This are files that return a hashtable. The hashtable format is documented on this cmdlet documentation.
			$algFilter = (PutFolderSlash $VALUES.SBA_FOLDER)+"*."+($VALUES.SBA_EXTENSION)
			Log "	FILES: $algFilter" "VERBOSE"
			gci $algFilter | %{
				Log "	Loading SOURCE BACKUP ALGORITHM $($_.FullName) " "VERBOSE"
				$AlgName = $_.Name.replace("."+$VALUES.SBA_EXTENSION,"")
				Log "	ALGORITHM IS: $AlgName" "VERBOSE"
				$VALUES.SCRIPT_STORE.SBA.add($AlgName,(& $_.FullName));
			}
			
			$SbaCount = $VALUES.SCRIPT_STORE.SBA.Count;
			
			if(!$SbaCount){
				throw "NO_SBA"
			}
			
			Log "	SBA Scripts: $SbaCount"
			
		#SQLInterface is just a way for organize calls to SQL Server commands.
			#This help for future changes on way how this cmdlet will connect to sql server...
		Log "Configuring SQL Interface..."
			$SQLInterface = @{	name="CUSTOMMSSQL"
								PreReq=$null
								cmdexec={param($S,$d,$Q,$U,$P,$i,[switch]$IgnoreExceptions = $false, [switch]$NoExecuteOnSuggest = $false)
											
											if($NoExecuteOnSuggest -and $SuggestOnly){
												Log "	NO EXECUTING BECAUSE IS IN SUGGESTONLY MODE" "VERBOSE"
												return;
											}

											$cmdparams = @{
												ServerInstance=$S
												Database=$d
											}
											if($U){
												$cmdparams.add("Login",$U)
												$cmdparams.add("Password",$P)
											}
																				
											if($Q){
												$cmdparams.add("Query",$Q)
											}
											
											if($i){
												$cmdparams.add("InputFile",$i)
											}

										try {
											$results = Invoke-NewQuery @cmdparams;	
											return $results;
										} catch {
											Log $_ 
											if(!$IgnoreExceptions){
												throw "MSSQL_ERROR: Last executed script failed! Check previous erros in log."
											}
										}
								}
							}
			$VALUES.SQLINTERFACE = $SQLInterface;

		#Lets collect some destination info in order guide script flow and validations.
		Log "Getting destination info"
			$ServerInfoCommand = . $VALUES.SCRIPT_STORE.SQL.GET_INSTANCE_INFO $VALUES
			$DestinatonInfo 	= . $SQLInterface.cmdexec -S $DestinationServerInstance -D master -Q $ServerInfoCommand
			$DestMajorVersion	=  GetProductVersionPart $DestinatonInfo.ProductVersion  1
			$DestMinorVersion	=  GetProductVersionPart $DestinatonInfo.ProductVersion  2
			$DestBuildNumber	=  GetProductVersionPart $DestinatonInfo.ProductVersion  3
			$DestVersion		=  GetProductVersionNumeric $DestinatonInfo.ProductVersion;

			
			$VALUES.DESTINATION_INFO.add("MajorVersion",$DestMajorVersion)
			$VALUES.DESTINATION_INFO.add("MinorVersion",$DestMinorVersion)
			$VALUES.DESTINATION_INFO.add("BuildNumber",$DestMinorVersion)
			$VALUES.DESTINATION_INFO.add("VersionNumeric",$DestVersion)
			$VALUES.DESTINATION_INFO.add("DatabaseExists",$DestinatonInfo.DestinationDatabaseExists)

			Log "	Destination version parts: NUMERIC: $DestVersion MAJOR: $DestMajorVersion MINOR: $DestMinorVersion BUILD: $DestBuildNumber"
			Log "	Destination database exists? $($VALUES.DESTINATION_INFO.DatabaseExists) "
			
			$SQL2008R2SPI = GetProductVersionNumeric "10.50.2500";
			if( $DestVersion -lt $SQL2008R2SPI  ) { #If before 2008 R2 SP1 (where sys.dm_os_volume_stats is available)
				$GetVolumesInSQL = $false
			}
			
		#Adding reporting info... In Suggest Only mode, a report is generated. The reporting information is on SUGGEST_REPORT value.
		$VALUES.SUGGEST_REPORT.add("Destination Instance",$VALUES.PARAMS.DestinationServerInstance)
		$VALUES.SUGGEST_REPORT.add("Destination Machine",$DestinatonInfo.ComputerName)
		$VALUES.SUGGEST_REPORT.add("Destination Database",$VALUES.PARAMS.DestinationDatabase)

		#At this points, lets get the source backup taht script will use to restore...
		Log "Getting source backup file"
		
			#If source instance and database are passed, we build the backup file prefix.
			#The backup file prefix is a file mask that will use to generate backup file and get backup file, if source is, for example, a existent backup file.
			if($SourceServerInstance -and $SourceDatabase){
				$ServerDBNameForFile = $SourceServerInstance.replace("\","_")+"."+$SourceDatabase.Replace("\","_");
				$BackupFilenameSuffix = "FORCOPY.bak"
				$VALUES.BACKUP_FILEPREFIX = $ServerDBNameForFile
				$VALUES.BACKUP_FILESUFFIX = $BackupFilenameSuffix
			}
		
			#This are the source scripts. Source Scripts are algorithsm used to get a backup source.
			#We will try invoke each SBA script in order specified by user. If it fails, we call next one.
			#When script returns the source path, we ends the source backup algorithm invoking!
			$ResultSBA = $null;
			foreach($SBALG in $VALUES.PARAMS.SourceOrder){
				$SBA = $VALUES.SCRIPT_STORE.SBA.$SBALG;
				
				if(!$SBA){
					continue;
				}
				
				Log "	Attempt use $($SBA.NAME)"
				$ResultSBA = & $SBA.SCRIPT $VALUES; #Call SBA script... note that outer script dont treats exceptions. Each script must wrry about rours errros...
				
				if($ResultSBA.fullPath){ #break the execution if a valid type is found (this assume that SBA returns a valid object!!!)
					$ResultSBA.algorithm = $SBALG;
					break;
				}
			
			}
			
			#If after the above part, no SOURCE BACKUP was determined, then abort!
			if(!$ResultSBA){
				throw "NO_SOURCE_BACKUP";
			}
			$VALUES.SOURCE_BACKUP = $ResultSBA;

		
		#If we have a source path, then we will store in in VALUES store and make some pre actions that are necessary.
		#When in suggest mode, this is not necessary, because no source path exists!
			if($VALUES.SOURCE_BACKUP.fullPath){ #If not in suggest mode. Suggest mode don't generate backup file.
				$fileName = [System.IO.Path]::GetFileName($VALUES.SOURCE_BACKUP.fullPath)
				$VALUES.SOURCE_BACKUP.originalFullPath = $VALUES.SOURCE_BACKUP.fullPath
				
				#If backup must be copied to another location, then do it.
				if($RemoteBackupFolder){
					Log "	Remote folder is specified. Copying backup to $($VALUES.PARAMS.RemoteBackupFolder)" "PROGRESS"
					
					$newBackupFilePath = (PutFolderSlash $VALUES.PARAMS.RemoteBackupFolder)+$fileName
					
					#In suggest mode, no copy is need. Then, this part of code must be executed only when SuggestOnly is disabled.
					if(!$VALUES.PARAMS.SuggestOnly){
						#If in normal mode, just copy the file and change backup file to new locaiton.
						copy $VALUES.SOURCE_BACKUP.fullPath $newBackupFilePath 
					}
					
					#If in suggest mode, just present in report the real final backup, but without chaning it...
					#Final backup will be changed because future commands will try acess it.
					$VALUES.SUGGEST_REPORT.add("FINAL_BACKUP_FILE",$newBackupFilePath)
					$VALUES.SOURCE_BACKUP.fullPath = $newBackupFilePath 
				}
				
				#Logging...
				Log "	Source backup file was determined: $($VALUES.SOURCE_BACKUP.fullPath)"
				$VALUES.SUGGEST_REPORT.add("ORIGINAL_PATH",$VALUES.SOURCE_BACKUP.originalFullPath);
				$VALUES.SUGGEST_REPORT.add("FINAL_PATH",$VALUES.SOURCE_BACKUP.fullPath);
			}
			
			
		#At this point, we need know which files are present in source backup for restore it correctly...
		Log "Determining files that need be restored..."
			
			#The dbfiles vars will recevei the list of files...
			[object[]]$dbfiles = @();
			
			#Lest determine files available using this SQL script...
			$FilesCommand = . $VALUES.SCRIPT_STORE.SQL.GET_BACKUP_FILES $VALUES
			Log "	FilesCommand: $FilesCommand"
			
			#Collect files to be restored dependes if Suggest only... If wll have a available backup file, use it, if not, connect to the source instance.
			if($VALUES.PARAMS.SuggestOnly -and $VALUES.SOURCE_BACKUP.type -eq "S"){
				$dbfiles = . $SQLInterface.cmdexec -S $VALUES.PARAMS.SourceServerInstance 		-d $ -Q $FilesCommand
			} else {
				$dbfiles = . $SQLInterface.cmdexec -S $VALUES.PARAMS.DestinationServerInstance 	-d master -Q $FilesCommand
			}
				
			
			#Mounting a standard file information object to work with it on this script.
			$filesInfo = @();
			$dbfiles | %{
				Log "	File $($_.LogicalName) have $($_.Size) bytes... "
				$filesInfo  += New-Object PsObject -Prop @{logicalName=$_.logicalName;size=$_.Size;restoreOn=$null;fullFileInfo=$_;pathIsComplete=$false}
			}
			
			if(@($filesInfo).count -eq 0){
				Log "	No database files information"
				throw "NO_FILES_INFO"
			}

		#Get destination files if destination database exists!
		if($VALUES.DESTINATION_INFO.DatabaseExists){
			Log "Getting available files in destination database..."
		
			#If destination database exists, get the information about files of database.
			CallCoreScript "DESTINATION_DATABASE_FILES";
		}
			
			
		#Lets now determine which volumes we are available to put database files...
		Log "Determining available volumes"
			$volumes = @();
		
			#The concept of "volume" on context of this script is a bit more extended...
			#Check the NewVolumeObject to more info...

			#The scripts try steps on this order: Manual file Mapping, Force Volume To Folder
			#If its fails and policy allows, then it try use another method. 
			# If direct mapping was specified, then it use it. If not, it uses the default, that is get remote volumes.
			
	
			#We use this information to filter which volume is allowed to log and data files...
			if($VolumesAllowed){
				$VolumesForData += $VolumesAllowed 
				$VolumesForLog += $VolumesAllowed
			}
			
			#There are many ways to determine available volumes to put database files.
			#Each if bellow represents a algorithm that get volumes using its respectevy way.

				if($NoRelocFiles){
					#Create a dummy volume just for represent this and passed next validations...
							$vol = NewVolumeObject;
							$vol.name ="DUMMY"
							$vol.freeSpace=0;
							$vol.volType = "DUMMY"
							$volumes += $vol;
							
					$VALUES.DFA_TOEXEC = "NORELOCFILES";
				}

				#Manual File Mapping
				if($ManualFileMapping -and !$volumes){
					Log "	Manual file mapping is choosed."
				
					$ManualFileMapping | %{
					
						if(!$_.logicalName){
							throw "EMPTY_LOGICAL_NAME"
						}
						
						if(!$_.physicalName){
							throw "EMPTY_PHYSICAL_NAME"
						}

						Log "	Manual file specified: $($_.logicalName) TO $($_.physicalName) "
							
						$vol = NewVolumeObject;
						$vol.name = $_.physicalName;
						$vol.sqlLogicalName = $_.logicalName;
						$vol.volType="SQLFILE"
						$volumes += $vol;
					}
					
					$VALUES.DFA_TOEXEC = "MANUAL_MAPPING";
				}
				
				#Try existen files;
				if(!$NoUseCurrentFilesFolder -and !$volumes){
					Log "	Attempting check current existents files as volumes"
					
					
					$CurrentFilesCommand = . $VALUES.SCRIPT_STORE.SQL.GET_CURRENT_FILES $VALUES
					try {
						$DestinationFiles = . $SQLInterface.cmdexec -S $DestinationServerInstance -d $DestinationDatabase -Q $CurrentFilesCommand
						
						if($DestinationFiles){
							$DestinationFiles | %{
								$vol = NewVolumeObject;
								$vol.name = $_.physicalName;
								$vol.sqlLogicalName = $_.logicalName;
								$vol.volType="SQLFILE"
								
								$volumes += $vol;
							}
						} else {
							throw "NO_DESTINATION_FILES"
						}
						
						#Empty the restore folder... This prevent a folder passed by user must be appended...
						$RestoreFolder = $null;
						$VALUES.DFA_TOEXEC = "REPLACE_EXISTENT";
						Log "		FILES COLLECTED SUCESSFULY!"
					} catch {
						Log $_
						Log "	Replace Policy is: $CurrentFilesFolderPolicy"
						
						if($CurrentFilesFolderPolicy -eq "MustReplace"){
							throw;
						}
						
						$NoUseCurrentFilesFolder = $true;
						$volumes = @();
					}
				}

				#Try forced mapping.
				if($ForceVolumeToFolder -and !$volumes){
					Log "	Using supplied folder paths as volumes..."
					
					$volumes = @()
					$VolumeToFolder | %{
						if($_.Path){
							$Path = $_.Path
						}
						$vol = NewVolumeObject;
						$vol.name = $path;
						$vol.volType="FOLDER"
						$volumes += $vol;
					}
					
					$VALUES.DFA_TOEXEC = "RANDOM_LEAST_USED";
				}
			
				if($GetVolumesInSQL -and !$volumes){
					Log "	Getting volumes using SQL DMVs... On destination instance"
					
					try {
						$GetVolumesCommand = . $VALUES.SCRIPT_STORE.SQL.GET_VOLUMES $VALUES
						$results = . $SQLInterface.cmdexec -S $DestinationServerInstance -d "master" -Q $GetVolumesCommand
						$results | %{
							$vol = NewVolumeObject;
							$vol.name = $_.volume_mount_point;
							$vol.freeSpace=$_.available_bytes;
							$vol.volType = "LUN"
							$volumes += $vol;
						}
					} catch {
						Log $_
						Log "	ERROR!. Traditional method will be used."
						$volumes = @();
					}
				}
				
				#Try remote volumes.
				if(!$volumes){
					Log "	Getting volumes available in remote server $($DestinatonInfo.ComputerName)"

					
					Get-WMiObject Win32_Volume -ComputerName $DestinatonInfo.ComputerName | where {$_.DriveType -eq 3 -and !$_.SystemVolume -and !$_.BootVolume} | %{
						$vol = NewVolumeObject;
						$vol.name = $_.name;
						$vol.freeSpace=$_.freeSpace;
						$vol.realFreeSpace=$_.freeSpace;
						$vol.volType = "LUN"
						$volumes += $vol;
						
						Log "	VOLUME FOUND: $($vol.name)"
						if($VolumesForData){
							Log "		Checking if this volume can hold data files..."
							$TmpVolumesAllowed = $VolumesForData | where {$vol.name -like $_}
							if($TmpVolumesAllowed){
								Log "			YES!"
								$vol.filesAllowed += "D"
							}
						}
						
						if($VolumesForLog){
							Log "		Checking if this volume can hold log files..."
							$TmpVolumesAllowed = $VolumesForLog | where {$vol.name -like $_};
							if($TmpVolumesAllowed){
								Log "			YES!"
								$vol.filesAllowed += "L"
							}
						}
					}
				}

	
			#At this point, we have collected all possible volumes... LEts log it...
			Log "	Volumes collected:"
			$volumes | %{
				Log "			$($_.Name) |TYPE: $($_.volType)"; 
			}
			
			#This represent all volume that we can use....
			$VolumesToUse = $VolumesForData + $VolumesForLog;
			
			#At this point we must eliminate volumes that user not specified.
			if($VolumesToUse){
				if($VALUES.DFA_TOEXEC -eq "REPLACE_EXISTENT"){
					Log "	Volumes allowed check will be ignored because current DFA."
				} else {
					$passedVolumes = @();
					Log "		 Was specified volume for filtering: "
					$VolumesToUse | %{Log "			$_"}
					Log "		Filtering volumes..."
					foreach($vol in $volumes){
						$passedFilters = $null;
						$passedFilters = $VolumesToUse | where {$vol.name -like $_}
					
						if(!$passedFilters){
							Log "			Volume $($vol.Name) was removed!"
							continue;
						}

						$passedVolumes += $vol
					}
					
					$volumes = $passedVolumes;
				}
			}

			#At this point we have all elegible volumes!
			$TotalVolumes = @($volumes).count
			if($TotalVolumes -eq 0){
				Log "	No volumes available for restore on destination server"
				throw "NO_VOLUMES_AVAILABLE"
			}

			Log "	Total Volumes: $TotalVolumes"	
			#Now, lets substract file size of each file already existent. 
			#This is because, if destination database exists, we must not consider space ocuppied by your files..
			#For each file, lets get corresponding volume and add free space and store it a new property...
			
			if($VALUES.DESTINATION_INFO.DatabaseExists){
				Log "	Adjusting volume free space based on existent files!"
				:FilesLoop foreach($CurrentFile in $VALUES.DESTINATION_INFO.DestinationFiles){
					#Get all files that starts with volume name (the path of volume)
					$ElegibleVolumes = @();
					
					:VolumesLoop foreach($CurrentVolume in $volumes){
						if($CurrentFile.physicalName -like ($CurrentVolume.name+"*")){
							#WE will create a object that contains the volume itselft and name length. We use must length to handle mount points problem.
							$ElegibleVolumes += New-Object PsObject -Prop @{VolumeObject=$CurrentVolume;NameLength=$CurrentVolume.name.length};
						}
					}
					
					#At this point, because of mount points, multiple volumes can be returned. We choose the volume which your name is the bigger length.
					$volumeToHandle = ($ElegibleVolumes | Sort-Object -Property NameLength -Descending | select -first 1).VolumeObject
					
					#Just reduce if vo
					if($volumeToHandle.volType -eq "LUN"){
						$volumeToHandle.freeSpace += ($CurrentFile.size*8*1024);
						Log "	Volume $($volumeToHandle.name) contains existing database file $($CurrentFile.physicalName). New free space: $($volumeToHandle.freeSpace) Real: $($volumeToHandle.realFreeSpace)"
					}
					
				}
			}
			
			$volumes | %{$VALUES.SUGGEST_REPORT.add("VOLUME: $($_.Name)","FREE_SPACE: $($_.FreeSpace) VIRTUAL_FREESPACE:$($_.realFreeSpace) TYPE: $($_.volType)") }

		#At this point, we can distribute files to volumes. Each DFA script handles your logic.
		#The DFA script that will be executed depends of parameters that users passed and the flow of script when determining volumes and files.
		if($NoRelocFiles){
			Log "No file relocation will be taken!"
		} else {
		
			Log "Calling distribute files algorithms... "

				#At this point, we can choose correct algortihm based on parameters...
				if(!$VALUES.DFA_TOEXEC){
					Log "	No valid DFA Script. Setting default..."
					$VALUES.DFA_TOEXEC = "BIGGERS_FIRST"
				}
				
				Log "	Choosed algorithm is: $($VALUES.DFA_TOEXEC) " "PROGRESS"
				
				#Retrieve the DFA scriptblock choosed.
				$DFAScript = $VALUES.DFA.Item($VALUES.DFA_TOEXEC)
			
				#Call the DFA!
				try {
					& $DFAScript $filesInfo $volumes
				} catch {
					throw "ERROR WHEN DISTIRUTING FILES: $_";
				}
		}
			
		Log "Preparing for restore on destination server..."

			$RestoreCommand = . $VALUES.SCRIPT_STORE.SQL.RESTORE_DATABASE $VALUES

			if(!$NoRelocFiles){
				Log "	Building MOVE CLAUSE"

				$RestoreFoldersChecked = @();
				$RemoteCheck = {
					param($FolderToCheck)
					$ErrorActionPreference= "Stop";
					
					if(-not(Test-Path $FolderToCheck)){
						$createdFolder = mkdir $FolderToCheck -force;
					}
				
				}
				
				foreach($info in $filesInfo){
					Log "		BUILDING FOR FILE '$($info.logicalName)'. Elegible volume is: $($info.restoreOn)"
					$destinationFile = $info.restoreOn;
					
					if(!$info.pathIsComplete){
						$info.restoreOn = (PutFolderSlash $info.restoreOn)
						$destinationFolder = $info.restoreOn;
						
						$mappedFolder = $VolumeToFolder | where { $destinationFolder -like $_.volumeName }
						
						if($mappedFolder){
							if( @($mappedFolder).count -gt 1  ){
								Log "		MULTIPLE MAPPED FOLDERS!!! FIX IT!"
								throw "MULTIPLE_MAPPED_FOLDERS"
							}
							
							$destinationFolder += $mappedFolder.path
						}
						elseif($RestoreFolder){
							$destinationFolder += $RestoreFolder;
						}
						
						#Check if restore folder exists on destination server... if not, attempt create...
						try {

							if($RestoreFoldersChecked -NotContains $destinationFolder){
								Log "		Trying create restore folder...: $destinationFolder";
								
								$RestoreFoldersChecked += $destinationFolder;
								$result = Invoke-Command -ScriptBlock $RemoteCheck -ArgumentList $destinationFolder -ComputerName $DestinatonInfo.ComputerName;
							}
							
						} catch {
							Log "			ERROR WHEN TRYING CREATE THE RESTORE FOLDER: $_";
						}
						
						$fileExt = ".ndf"
						
						switch ($info.fullFileInfo.Type){
							"L" {$fileExt = ".ldf"}
							"D" {
									if($info.fullFileInfo.FileID -eq 1){
										$fileExt = ".mdf"
									}
								}
							"F" {$fileExt = ""}
							"S" {$fileExt = ""}
							default {$fileExt = ".unkext"}
						}
						
						$filePath = [System.IO.Path]::GetFileName($info.fullFileInfo.PhysicalName);
						$logicalName = $info.LogicalName.replace("\","_")
						$destinationFile = $destinationFolder+"$DestinationDatabase.$logicalName$fileExt"
					}
					
					Log "			Destination is: $destinationFile"
					$moveTO = ",MOVE '$($info.logicalName)' TO '$destinationFile' "
					
					$VALUES.SUGGEST_REPORT.add("FILE: $($info.logicalName)","TO: $destinationFile");
					
					$RestoreCommand += "`t`t`t"+$moveTO+"`r`n"
				}
			}
			
		

		
		if($DestinationDatabaseBackup -and $VALUES.DESTINATION_INFO.DatabaseExists){
			Log "Destination database will be backed!"
			
			$ts = (Get-Date).toString("yyyy-MM-dd-HHmmss");
			$ServerDBNameForFile = $DestinationServerInstance.replace("\","_")+"."+$DestinationDatabase.Replace("\","_");
			
			$DestinationBackupPath = (PutFolderSlash $DestinationDatabaseBackup)+$ServerDBNameForFile+".$ts.bak"
			Log "	Backup destination: $DestinationBackupPath" "PROGRESS"
			
			$BackupOptions = ""
			
			#Determining options...
				if($DestMajorVersion -ge 9){ #If 2005 or high...
					$BackupOptions += ",COPY_ONLY"
				}
				
				if($DestMajorVersion -ge 10){ #If 2008 or high
					$BackupOptions += ",COMPRESSION"
				}
			
			
			$BackupDestinationCommand = . $VALUES.SCRIPT_STORE.SQL.BACKUP_DESTINATION $VALUES
			
			$VALUES.SUGGEST_REPORT.add("DESTINATION BACKUP: $($info.logicalName)","TO: $DestinationBackupPath");
			
			Log "	Backup Destination command: $BackupDestinationCommand"
			$status = & $SQLInterface.cmdexec -S $DestinationServerInstance -d master -Q $BackupDestinationCommand -NoExecuteOnSuggest
			Log "	$($status.Status)" "PROGRESS"
		}	
		
		if($KeepPermissions -and $VALUES.DESTINATION_INFO.DatabaseExists){
			Log "	The permissions will remain after restore."
			
			CallCoreScript "DESTINATION_PERMISSION_BACKUP" -LogErrorOnly;
		}
		
		Log "Restoring databases on destination..."

			$KillConnectionCommand = . $VALUES.SCRIPT_STORE.SQL.KILL_CONNECTIONS $VALUES
			
			Log "	Kill Connections command: $KillConnectionCommand"
			Log "	Restore command: $RestoreCommand "


			& $SQLInterface.cmdexec -S $DestinationServerInstance -d master -Q $KillConnectionCommand -NoExecuteOnSuggest
			& $SQLInterface.cmdexec -S $DestinationServerInstance -d master -Q $RestoreCommand	-NoExecuteOnSuggest	
			
		if($ForceSimple){
			Log "Putting database in SIMPLE RECOVERY"
			try {
				$Command = "ALTER DATABASE [$DestinationDatabase] SET RECOVERY SIMPLE WITH ROLLBACK IMMEDIATE;"
				& $SQLInterface.cmdexec -S $DestinationServerInstance -d master -Q $Command  -NoExecuteOnSuggest
				Log "	Success!"
			} catch {
				Log $_ 
				Log "	ERROR:  $ErrorMessage" "PROGRESS"
			}
		}
			
		
		if($KeepPermissions -and $VALUES.DESTINATION_INFO.DatabaseExists){
			Log "	The backed up permissions are going to restore"
			
			CallCoreScript "RESTORE_PERMISSION" -LogErrorOnly;
		}
			
			
			
		if($PostScripts){
			Log "Posts scripts are requested"
			
			#Some auxiliary variables...
				$ScriptOrder = 0;
				$ExpandedScripts = @()
				#Preparing parameters to execute script via SQLInterface...
				$connectParams = @{S=$VALUES.PARAMS.DestinationServerInstance;d=$VALUES.PARAMS.DestinationDatabase;i=$null;Q=$null;NoExecuteOnSuggest=$true};
			
			$DropLimitedUser = $false;
			if($UseLimitedUser){
				Log "	A Limited user will be used!" "PROGRESS"
				$GenerateLimitedUserCommand = & $VALUES.SCRIPT_STORE.SQL.CREATE_LIMITED_USER $VALUES
				& $SQLInterface.cmdexec -S $DestinationServerInstance -d $DestinationDatabase -Q $GenerateLimitedUserCommand -NoExecuteOnSuggest
				$DropLimitedUser = $true;
				$LimitedName = $VALUES.LIMITED_USER.NAME;
				$LimitedPass = $VALUES.LIMITED_USER.PASSWORD;
				Log "	LIMITED USER DETAILS: $LimitedName PASS: $LimitedPass" "VERBOSE"
				$VALUES.SUGGEST_REPORT.add("LIMITED_USER","NAME:$LimitedName PASS:$LimitedPass")
				$connectParams.add("U",$LimitedName)
				$connectParams.add("P",$LimitedPass)
			}


			#Expanding scripts.
			#	Expading do: If scripts is prefixed with FILE:<PATH> and path is a folder, then scritps get all files recursively.
			#					TODO: Expand powershell scriptblocks...
			Log "	Analyzing post scripts (expanding)..."
			foreach($Script in $PostScripts){
				try {
					$o = New-Object PSObject -Prop @{
								scriptType 		= $null
								scriptContent	= $null
								scriptOrder		= $ScriptOrder++
							}
							
					#Lets determine the path. If PostScript stats with FOLDER: or FILE: then treats the string after this as a path. 
					$IsFileExpression = $Script -match "^(file):.+$"
					$IsFolderExpression =  $Script -match "^(folder):.+$"
					$IsPathExpression = $IsFolderExpression -or $IsFileExpression;

					if($IsPathExpression)
					{
						$FileName = $Script -replace '^[^:]+:',"";
	
						#If is FILE: and path is a directory...
						if($IsFileExpression){
							if( IsDirectory($FileName) ){
								$IsExpandedFiles = $true;
							}
						}
						
						if($IsFolderExpression){
							$IsExpandedFiles = $true;
						}
						
					
						if( $IsExpandedFiles ){
							Log "	Folder Detected: $FileName" "DETAILED"
							
							$ChildFiles = gci -recurse $FileName | where {!$_.PSIsContainer} 
							$Objects = @();
							
							$ChildFiles | sort -Property Name | %{
								Log "		File detected: $($_.FullName)" "DETAILED"
								$oCopy = $o.psobject.copy();
								$oCopy.scriptType 		= "FILE";
								$oCopy.scriptContent 	= $_.FullName;
								$oCopy.scriptOrder 		= $ScriptOrder++;
								$Objects += $oCopy
							}
							
							$o = $Objects;
						} else {
							$o.scriptType = "FILE"
							$o.scriptContent = $FileName
						}
					} else {
						$o.scriptType = "SQL"
						$o.scriptContent = $Script 
					}
					
					$ExpandedScripts += $o
				} catch {
					$message = $_.Exception.GetBaseException().Message
					Log "		ERROR: $message" "PROGRESS"
				}
			}

			
			#Executing scripts...
			Log "	Starting post scripts applying..."
			$ExpandedScripts | sort ScriptOrder | %{
				$Script = $_;
				Log "	Applying post script $($Script.ScriptOrder): $($Script.scriptContent)" "PROGRESS"

				$VALUES.SUGGEST_REPORT.add("POST_SCRIPT $($Script.ScriptOrder)","$Script.scriptContent")
				
				#Remove exclusive parameters...
				if($connectParams.Contains("Q")){$connectParams.remove("Q");}
				if($connectParams.Contains("i")){$connectParams.remove("i");}
				
				#Determining if command to execute is a input file or query string...
				if($Script.scriptType -eq "FILE"){
					Log "		Script is a input file!" "VERBOSE"
					$connectParams.add("i",$Script.scriptContent)
				} else {
					$connectParams.add("Q",$Script.scriptContent);
				}
					
				#Execute and HANDLE errors.
				try {
					$SucessExecuted = $false;
					$result = @(& $SQLInterface.cmdexec @connectParams)			
					$SucessExecuted = $true;
					
					Log "	SUCCESS!" "PROGRESS";
					
					$result | where {$_} | %{	
						$resultString = Object2HashString($_)
						Log "		$resultString"
					}
						
				} catch {
					#Catch and handle script execution errors...
					$message = "ERROR!"
					if($SucessExecuted){$message = "ERROR_AT_RESULTS_DISPLAYING"}
				
					Log "	$message POLICY: $PostScriptPolicy" "PROGRESS"
					Log $_
					
					if($PostScriptPolicy -eq "StopOnError"){throw "POST_SCRIPT_ERROR"}
				}
			}
			
			#If LimitedUser must be dropped...
			if($DropLimitedUser){
				Log "DROPING LIMITED USER..."
				$DropLimitedUserCommand = & $VALUES.SCRIPT_STORE.SQL.DROP_LIMITED_USER $VALUES
				& $SQLInterface.cmdexec -S $DestinationServerInstance -d $DestinationDatabase -Q $DropLimitedUserCommand -IgnoreExceptions -NoExecuteOnSuggest
			}
		
		}
		
		if($SuggestOnly){
			write-host "================= SUGGEST REPORT ================= "
			$VALUES.SUGGEST_REPORT.GetEnumerator() | sort Key | %{
				write-host "$($_.Key) $($_.Value)"
			}
		} else {
		
			Log "DATABSE COPIED!" "PROGRESS"
			Log "	SOURCE: $SourceServerInstance $SourceDatabase File:$($VALES.SOURCE_BACKUP.fullPath)" "PROGRESS"
			Log "	DESTINATION: $DestinationServerInstance $DestinationDatabase" "PROGRESS"	
		}
		
		#Finalizations....
		$ScriptExitCode = 0;
		$ExitWithCode = $false; #TODO: Transform in a parameter...
		Log "SCRIPT EXECUTED SUCESSFULLY";
	} catch {
		#If SQLAgent mode is on, then the script finalize the exit code other than 0 is propagate to caller process.
		#If it not happens, the powershell process end without passing exit code, and caller process dont knows nothing about the error. in this case, jobs present sucess.
		#With this solution, script ends with a error code different than 0, that will be passed to calling process (sql agent) that can fail the job on history...
		if($SQLAgentErrorMode){
			$ScriptExitCode = 1;
			$ExitWithCode = $true;
		}
		
		if(Get-Command -Name Log -EA "SilentlyContinue"){
			Log $_;
		}
		
		throw;
	} finally {
		if($Log.OutBuffer -and $BufferLog){
			write-host @(($Log.OutBuffer) -join "`r`n")
		}
		
		if($ScriptExitCode -and $ExitWithCode){
			exit $ScriptExitCode;
		}
	}
		
	<#
		.SYNOPSIS 
			Copies database to another (or same) instance using backup/restore method.
			
		.DESCRIPTION
			This cmdlet allow you duplicate databases, by copying to a destination instance.
			You must provider source and destination information.
			
			SPECIFYING A SOURCE
				There are multiple ways to get a source backup for copy. We call the source backup algorithm, or SBA.
				On each release of this script, this can change. Always check documentation and parameters.
				The set of parameters used for source depends on source used.
				
					SQL SERVER SOURCE (or real time BACKUP)
					
						This source is simple: Just generate e backup when scripts runs (COPY_ONLY and COMPRESSION are used if supported).
						The parameter BackupFolder will specify where backup file will be placed.
						The filename generated will be  $SourceServerInstance.$SourceDatabase[.$UNIQUETS].FORCOPY.bak.
						If $UniqueBackupName option is specified, then a unique string is used in place on $UNINQUETS. If not, this part of name is not used.
						For use this parameter, you must provide SourceServerInstance and SourceDatabase parameters and any additional logon information associated with this.
						If cannot connect on Source SQL Server and take source backup, this source fails.
						
					RECENTLY BACKUP SOURCE
					
						This source is allows you specify a most recently backup for use.
						When this source is used, the cmdlets lookup specified folder and select the most recently file.
						The cmdlet take care for lookup for just files that matchs default pattern generated by this scripts in SQL SERVER SOURCE.
						You can change this behavior by specifying the RecentFileMask parameter.
						You can use RecentBase parameter for specify base time. Just files with LastWriteTime grather than or equals this time will be considered.
						You can use (Get-Date).addHours(-24), for example, to specify Last 24 hours. If most recently backup have 25 hours, then no recent backup will be taken.
						For enable this source, you must use UseRecentBackup parameter.
						If no file is elegible, this source files.
						The paramater UseExistentPolicy controls if this source will fail entire scripts if this source fails.
						
					EXISTENT BACKUP Source
					
						This source allows you specify a filename to use.
						Just specify filename in ExistentBackupName and cmdlet will try gets this backup as source.
						If file dont exists, this source fails.
						The paramater UseExistentPolicy controls if this source will fail entire scripts if this source fails.
				
				
			SPECIFYING A DESTINATION
			
				The destination is allways a SQL Server instance. The cmdlets will issue a RESTORE command on it.
				You specify by using parameters $DestinationServerInstance and $DestinationDatabase.
				
				The cmdlet will try RESTORE database based on Source Backup choosed.
				You can specify a alternate restore folder from which RESTORE COMMAND will be issued with RemoteBackupFolder.
				With this command, the Source backup determined will be copied to this folder before restore command.
				This can be useful when BackupFolder is slow for destination instance.
				The RestoreFolder is used for determine in which folder database files must be put on (MOVE CLAUSE)
				
				This cmdlets provide options for database after restore.
				You can force database in SIMPLE recovery by using ForceSimple parameter.
				You can stay database in NORECOVERY with NoRecovery parameter.
				
				Also, you can specify custom scripts to be applied on database after restore. See "POST SCRIPTS" later.
				
			CONTROLING MOVE CLAUSE
			
				One of big vantages of this scripts, is a hability of choose and determine MOVE clause.
				This happens thanks to distribution files algorithms (we call DFA). Check more details about DFA on "DFA Architecture" later.
				Basically, for build MOVE clause, the cmdlet must determine VOLUMES and FILES.
				
					THE FILES
						
						Files are simple to understand: It are the database files. This information can be retrivied just by using source backup file.
						The cmdlet use RESTORE FILELISTONLY for determine files. It can use sys.database_files on source in some cases.
						If cmdlet cannot determine source files, it fails and show reason.
						
					THE VOLUMES 
					
						Volumes are a bit more complex concept on this cmdlet. You can think that volume, in this context, is a destination of a FILE.
						It can be, for example, volume, folder, etc. The type of volume will be determined based on parameters that you specify.
						For example, when you specify NoUseCurrentFilesFolder, the possible destinations are the disk volumes (mount points, drive letters, etc.) on destination instance.
						When you dont specify this parameter (the default) the possible destinations are the folders on destination database files is placed.
						You can check more detailed information on "DFA Architecture" section.
						The script will determine the volumes based on parameters specified.
						
				
					
				You can specify which volumes the script will must consider for restore by using VolumesAllowed. Just volumes in this list will be considered.
				If you want control different volumes for data and log, you can use VolumesForData and VolumesForLog parameters.
				If you knows the best file mapping, you can force manual Move Mapping with ManualFileMapping parameter.
				

				
				
			POST SCRIPTS
			
				You can apply custom scripts on restored database.
				The parameter PostScript is a array o string, where each string specify the script to be applied.
				You can use FILE: prefix on string. The cmdlet will consider the following string be a filename.
				If you specify FOLDER: prefix, the cmdlet will consider the following string be a folder. You can use wildcards. For Example "FOLDER:D:\SQL\Restore\MyDb*.sql"
				If you dont specify a prefix, the cmdlet will consider the string being the TSQL command to apply.
				
				The scripts are applied in order which are specified. For FOLDER: the order is maintaned. The scripts applied elegible files ordered by name.
				The PostScriptPolicy controls if scripts must be continue applied on error.
				
				
			
			DFA ARCHITECURE
			
				DFA is mechanism by the this cmdlet will determine where a file will be restored on.
				In DFA, there are two main concepts: FILES and VOLUMES.
				FILES are the database files.
				VOLUMES are the places where FILES can be restored.
				
				The macro DFA operation is determine files and volumes, and distribute them. Based on this, the move clause is build.
				Determining files is simple task. DFA code must get source backup and use RESTORE FILELISTONLY for determine which files it need restore.
				Determine volumes is a bit more complex.
				
					DETERMINING Volumes
					
						The files can be restored in most places:
							- In volumes with available space on server where destination server is.
							- In the same place on a possible existent destination database.
							- In same place as original file
							- In specific folder locations, depending on drive letter, or mount point name.
							
					This is completely variable case, e can change between releases of this cmdlet.
					This source of volumes can be (this list can change):
						
							EXISTENT Files
								In this way, the cmdlet will query sys.database_files of destination database.
								Each returned file will be a possible destination.
							
							MANUAL MAPPING
							
								No volume is need be determined.
								The user must specify mapping of each file. 
								
							FORCED FOLDERS
							
								The source are folders with no information.
								User must specify the folders. This can be useful when script cannot have permissions for get folders.
								
							VOLUMES
							
								The source are volumes on destination server where destination instance are on.
								The scripts will discovery volume and free space information.
						
				
			
			
		.EXAMPLE
			Copy-SQLDatabase -SourceServerInstance Instance1 -SourceDatabase Database1 -DestinationServerInstance Instance2 -DestinationDatabase Database1_Copy -BackupFolder \\TEMP\SQLbackups
			
			In this example we simply copy database Database1, at Instance1 to Instance2 with name Database1_Copy.
			The backups will be made on \\TEMP\SQLbackups folder. This folder also will be used to restore...
			The files on database will be distributed on availble volumes on destination instance machine.
			
		.NOTES
		
			KNOW BUGS AND TO DO LIST:
				- TODO: Allow user specify a separate location for post script execution results.
				- TODO: Implement throttling. Reconsider this name, because is to dificult write....
				- TODO: Implement locked file access enchament.

	#>
}	



#Returns a object that represent a volume object...
			#	Volumes can be anything that a database file can be restore on.
			#	for example, it can be a Disk (LUN), a exitent file to be replaced, a folder or file mapped directly by user
			#For simplify, this scripts call all this as a "volume".
			#This script expects the volume objects have following properties:
				# name: path to the volume (depends of type)
				# freeSpace: bytes free on volume. some volumes types dont have this property
				# sqlLogicalName (name of logical sql file associated with volume, when existing files is used)
				# filesAllowed: Type of database files that is volume reserverd for. (LOG,DATA,ANY)
				# volType (type of volume): Must be: 
				#		LUN: are disk representations. It can be disk Letter or Mount Point. It is same original volume concept. It must have freeSpace information.
				#		FOLDER: are folder representations. This type is used when file will be restored on a specific folder. The script will generate right filename. Because freeSpace cannot be determine on folders, this type of volume dont have freeSpace property.
				#		SQLFILE: This type indicates the volume is a real existent sqlfile name. The name property is the physicalName of file.
				#		DUMMY: A dummy volume that will no be used for any thing.
Function NewVolumeObject {
	$o = New-Object PsObject -Prop @{
			freeSpace=$null
			name=$null
			sqlLogicalName=$null
			volType=$null
			filesAllowed=@()
			realFreeSpace=$null
		}
		
	$isFileAllowedMethod = [scriptblock]::create({
			param($fileInfo)
				
				
			if(!$this.filesAllowed){
				return $true;
			}	
			

			return $this.filesAllowed -Contains $fileInfo.fullFileInfo.type; 
		})
		
	$o | Add-Member -Type ScriptMethod -Name isFileAllowed -Value $isFileAllowedMethod;
		
	return $o;
}


#Returns a object that represent a SOURCE BACKUP
Function NewSourceBackup {

	$o = New-Object PsObject -Prop @{
			type = "FILE" #Valid are: FILE (Is a backup file)
			name = $null
			fullPath=$null
			originalFullPath=$null
			algorithm="" #Algorithm that determine the source. the acceptable values are same as $SourceOrder parameter. Valid values are same name of file that implements algorithms...
		}
		

	return $o;
}